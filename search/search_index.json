{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Project overview","text":"<p>Welcome to the comprehensive documentation landscape of Sotex-Box, where the hardware meets the software in a symphony of innovative advertising. This documentation serves as your guide, meticulously crafted to illuminate the intricate details of our ambitious endeavor.</p>"},{"location":"index.html#a-tale-of-two-worlds","title":"A Tale of Two Worlds","text":"<p>Sotex-Box bridges the gap between the tangible realm of physical devices and intangible realm of software, orchestraing a seamless dance between the two. This documentation reflects this duality, catering to both the hardware engineers who breathe life into the physical device and the software maestros who craft the digital experience.</p>"},{"location":"index.html#navigating-the-roadmap","title":"Navigating the Roadmap","text":"<p>Within these pages, you'll find meticulously arranged sections, each serving a distinct purpose:</p> <ul> <li>System Overview: Unveiling the project's grand vision, its intended audience, and the groundbreaking value it delivers.</li> <li>Hardware Design: Delving into the meticulous details of the device's form and function, exploring its components and intricacies.</li> <li>Software Architecture: Charting the inner workings of the software ecosystem, unveiling its components, interactions and data flow.</li> <li>Development process: Witnessing the meticulous steps one should take during development, from concept to reality, using industry-leading practices and methodologies.</li> <li>Deployment: Ensuring the smooth transition from development to the real world, with detailed deployment instructions.</li> </ul>"},{"location":"index.html#beyond-the-pages","title":"Beyond the pages","text":"<p>Remember, this documentation is an ever-evolving companion, continously updated to reflect the project's ongoing development and adaptation. Consider it a springboard for further exploration, a foundation for your understanding and a testament to the collaborative spirit that drives us forward.</p> <p>Whether you're a seasoned engineer delving into the technical intricacies or a curious stakeholder seeking a broader understanding, we invite you to explore these pages. They hold the key to unlocking the potential of Sotex-box and realizing its groundbreaking vision.</p> <p>Happy reading, and welcome to the project!</p>"},{"location":"benchmark-EventCoordinator.html","title":"EventCoordinator","text":""},{"location":"benchmark-EventCoordinator.html#benchmarks-with-keys-as-basic-strings","title":"Benchmarks with keys as basic strings","text":"<pre><code>BenchmarkDotNet v0.13.12, Ubuntu 22.04.4 LTS (Jammy Jellyfish)\n12th Gen Intel Core i7-12700H, 1 CPU, 20 logical and 14 physical cores\n.NET SDK 8.0.200\n  [Host]     : .NET 8.0.2 (8.0.224.6711), X64 RyuJIT AVX2\n  DefaultJob : .NET 8.0.2 (8.0.224.6711), X64 RyuJIT AVX2\n</code></pre> Method InitialSize Implementation amount Mean Error StdDev Median Gen0 Gen1 Allocated Add_N 100 EventCoordinatorConcurrentDictionary 10 6,700.00 ns 402.043 ns 1,166.400 ns 6,502.30 ns 0.2899 - 3712 B Add_N 100 EventCoordinatorConcurrentDictionary 100 21,493.46 ns 676.503 ns 1,994.684 ns 21,269.50 ns 1.0986 - 14107 B Add_N 100 EventCoordinatorConcurrentDictionary 1000 56,868.17 ns 1,108.665 ns 1,037.046 ns 56,672.22 ns 10.0098 0.1221 126309 B Add_One 100 EventCoordinatorConcurrentDictionary ? 53.75 ns 0.975 ns 1.334 ns 53.32 ns 0.0076 - 96 B Add_N 100 EventCoordinatorMutex 10 4,995.22 ns 188.683 ns 556.337 ns 4,807.59 ns 0.2594 - 3309 B Add_N 100 EventCoordinatorMutex 100 18,878.24 ns 726.818 ns 2,143.039 ns 18,731.93 ns 0.8240 - 10566 B Add_N 100 EventCoordinatorMutex 1000 38,567.99 ns 765.102 ns 1,145.168 ns 38,331.85 ns 7.5684 0.0610 94441 B Add_One 100 EventCoordinatorMutex ? 17.49 ns 0.034 ns 0.032 ns 17.50 ns 0.0051 - 64 B Add_N 100 EventCoordinatorReaderWriterLock 10 6,437.93 ns 219.122 ns 642.648 ns 6,198.87 ns 0.2899 - 3766 B Add_N 100 EventCoordinatorReaderWriterLock 100 NA NA NA NA NA NA NA Add_N 100 EventCoordinatorReaderWriterLock 1000 NA NA NA NA NA NA NA Add_One 100 EventCoordinatorReaderWriterLock ? 51.04 ns 0.311 ns 0.276 ns 51.10 ns 0.0076 - 96 B Add_N 10000 EventCoordinatorConcurrentDictionary 10 6,737.37 ns 219.005 ns 645.741 ns 6,562.13 ns 0.3281 - 4171 B Add_N 10000 EventCoordinatorConcurrentDictionary 100 22,448.28 ns 712.412 ns 2,100.564 ns 22,358.23 ns 1.3428 - 17058 B Add_N 10000 EventCoordinatorConcurrentDictionary 1000 59,378.91 ns 1,184.528 ns 3,182.158 ns 58,801.66 ns 10.4980 - 133028 B Add_One 10000 EventCoordinatorConcurrentDictionary ? 62.00 ns 0.220 ns 0.195 ns 62.04 ns 0.0101 - 128 B Add_N 10000 EventCoordinatorMutex 10 5,464.49 ns 225.515 ns 650.661 ns 5,190.12 ns 0.2899 - 3683 B Add_N 10000 EventCoordinatorMutex 100 20,463.57 ns 706.944 ns 2,084.439 ns 20,285.31 ns 1.0986 - 13795 B Add_N 10000 EventCoordinatorMutex 1000 40,370.36 ns 786.752 ns 841.816 ns 40,579.07 ns 8.0566 0.1221 100818 B Add_One 10000 EventCoordinatorMutex ? 27.84 ns 0.145 ns 0.129 ns 27.87 ns 0.0076 - 96 B Add_N 10000 EventCoordinatorReaderWriterLock 10 6,701.33 ns 235.912 ns 695.593 ns 6,364.40 ns 0.3204 - 4066 B Add_N 10000 EventCoordinatorReaderWriterLock 100 NA NA NA NA NA NA NA Add_N 10000 EventCoordinatorReaderWriterLock 1000 NA NA NA NA NA NA NA Add_One 10000 EventCoordinatorReaderWriterLock ? 63.96 ns 0.172 ns 0.161 ns 63.99 ns 0.0101 - 128 B Add_N 100000 EventCoordinatorConcurrentDictionary 10 6,293.26 ns 236.646 ns 694.041 ns 6,013.30 ns 0.3281 - 4107 B Add_N 100000 EventCoordinatorConcurrentDictionary 100 22,452.06 ns 619.333 ns 1,826.117 ns 22,555.13 ns 1.4343 - 18046 B Add_N 100000 EventCoordinatorConcurrentDictionary 1000 52,098.20 ns 1,033.150 ns 1,889.173 ns 52,411.59 ns 11.2305 - 141306 B Add_One 100000 EventCoordinatorConcurrentDictionary ? 67.44 ns 0.302 ns 0.252 ns 67.39 ns 0.0107 - 136 B Add_N 100000 EventCoordinatorMutex 10 4,724.82 ns 75.103 ns 80.359 ns 4,705.35 ns 0.2899 - 3643 B Add_N 100000 EventCoordinatorMutex 100 18,839.55 ns 581.042 ns 1,713.215 ns 18,361.50 ns 1.1597 - 14548 B Add_N 100000 EventCoordinatorMutex 1000 39,517.13 ns 788.836 ns 1,422.433 ns 39,263.24 ns 8.7280 0.1221 108882 B Add_One 100000 EventCoordinatorMutex ? 27.40 ns 0.061 ns 0.051 ns 27.41 ns 0.0083 - 104 B Add_N 100000 EventCoordinatorReaderWriterLock 10 6,893.68 ns 239.329 ns 698.135 ns 6,715.11 ns 0.3357 - 4272 B Add_N 100000 EventCoordinatorReaderWriterLock 100 22,724.46 ns 647.550 ns 1,909.316 ns 22,648.35 ns 1.4343 - 18126 B Add_N 100000 EventCoordinatorReaderWriterLock 1000 NA NA NA NA NA NA NA Add_One 100000 EventCoordinatorReaderWriterLock ? 59.38 ns 0.193 ns 0.171 ns 59.38 ns 0.0107 - 136 B <p>Benchmarks with issues:   Benchmarks.Add_N: DefaultJob [InitialSize=100, Implementation=EventCoordinatorReaderWriterLock, amount=100]   Benchmarks.Add_N: DefaultJob [InitialSize=100, Implementation=EventCoordinatorReaderWriterLock, amount=1000]   Benchmarks.Add_N: DefaultJob [InitialSize=10000, Implementation=EventCoordinatorReaderWriterLock, amount=100]   Benchmarks.Add_N: DefaultJob [InitialSize=10000, Implementation=EventCoordinatorReaderWriterLock, amount=1000]   Benchmarks.Add_N: DefaultJob [InitialSize=100000, Implementation=EventCoordinatorReaderWriterLock, amount=1000]</p>"},{"location":"benchmark-EventCoordinator.html#benchmarks-with-keys-as-guid","title":"Benchmarks with keys as Guid","text":"<pre><code>BenchmarkDotNet v0.13.12, Ubuntu 22.04.4 LTS (Jammy Jellyfish)\n12th Gen Intel Core i7-12700H, 1 CPU, 20 logical and 14 physical cores\n.NET SDK 8.0.200\n  [Host]     : .NET 8.0.2 (8.0.224.6711), X64 RyuJIT AVX2\n  DefaultJob : .NET 8.0.2 (8.0.224.6711), X64 RyuJIT AVX2\n</code></pre> Method InitialSize Implementation amount Mean Error StdDev Median Gen0 Gen1 Allocated Add_N 100 EventCoordinatorConcurrentDictionary 10 6,700.00 ns 402.043 ns 1,166.400 ns 6,502.30 ns 0.2899 - 3712 B Add_N 100 EventCoordinatorConcurrentDictionary 100 21,493.46 ns 676.503 ns 1,994.684 ns 21,269.50 ns 1.0986 - 14107 B Add_N 100 EventCoordinatorConcurrentDictionary 1000 56,868.17 ns 1,108.665 ns 1,037.046 ns 56,672.22 ns 10.0098 0.1221 126309 B Add_One 100 EventCoordinatorConcurrentDictionary ? 53.75 ns 0.975 ns 1.334 ns 53.32 ns 0.0076 - 96 B Add_N 100 EventCoordinatorMutex 10 4,995.22 ns 188.683 ns 556.337 ns 4,807.59 ns 0.2594 - 3309 B Add_N 100 EventCoordinatorMutex 100 18,878.24 ns 726.818 ns 2,143.039 ns 18,731.93 ns 0.8240 - 10566 B Add_N 100 EventCoordinatorMutex 1000 38,567.99 ns 765.102 ns 1,145.168 ns 38,331.85 ns 7.5684 0.0610 94441 B Add_One 100 EventCoordinatorMutex ? 17.49 ns 0.034 ns 0.032 ns 17.50 ns 0.0051 - 64 B Add_N 100 EventCoordinatorReaderWriterLock 10 6,437.93 ns 219.122 ns 642.648 ns 6,198.87 ns 0.2899 - 3766 B Add_N 100 EventCoordinatorReaderWriterLock 100 NA NA NA NA NA NA NA Add_N 100 EventCoordinatorReaderWriterLock 1000 NA NA NA NA NA NA NA Add_One 100 EventCoordinatorReaderWriterLock ? 51.04 ns 0.311 ns 0.276 ns 51.10 ns 0.0076 - 96 B Add_N 10000 EventCoordinatorConcurrentDictionary 10 6,737.37 ns 219.005 ns 645.741 ns 6,562.13 ns 0.3281 - 4171 B Add_N 10000 EventCoordinatorConcurrentDictionary 100 22,448.28 ns 712.412 ns 2,100.564 ns 22,358.23 ns 1.3428 - 17058 B Add_N 10000 EventCoordinatorConcurrentDictionary 1000 59,378.91 ns 1,184.528 ns 3,182.158 ns 58,801.66 ns 10.4980 - 133028 B Add_One 10000 EventCoordinatorConcurrentDictionary ? 62.00 ns 0.220 ns 0.195 ns 62.04 ns 0.0101 - 128 B Add_N 10000 EventCoordinatorMutex 10 5,464.49 ns 225.515 ns 650.661 ns 5,190.12 ns 0.2899 - 3683 B Add_N 10000 EventCoordinatorMutex 100 20,463.57 ns 706.944 ns 2,084.439 ns 20,285.31 ns 1.0986 - 13795 B Add_N 10000 EventCoordinatorMutex 1000 40,370.36 ns 786.752 ns 841.816 ns 40,579.07 ns 8.0566 0.1221 100818 B Add_One 10000 EventCoordinatorMutex ? 27.84 ns 0.145 ns 0.129 ns 27.87 ns 0.0076 - 96 B Add_N 10000 EventCoordinatorReaderWriterLock 10 6,701.33 ns 235.912 ns 695.593 ns 6,364.40 ns 0.3204 - 4066 B Add_N 10000 EventCoordinatorReaderWriterLock 100 NA NA NA NA NA NA NA Add_N 10000 EventCoordinatorReaderWriterLock 1000 NA NA NA NA NA NA NA Add_One 10000 EventCoordinatorReaderWriterLock ? 63.96 ns 0.172 ns 0.161 ns 63.99 ns 0.0101 - 128 B Add_N 100000 EventCoordinatorConcurrentDictionary 10 6,293.26 ns 236.646 ns 694.041 ns 6,013.30 ns 0.3281 - 4107 B Add_N 100000 EventCoordinatorConcurrentDictionary 100 22,452.06 ns 619.333 ns 1,826.117 ns 22,555.13 ns 1.4343 - 18046 B Add_N 100000 EventCoordinatorConcurrentDictionary 1000 52,098.20 ns 1,033.150 ns 1,889.173 ns 52,411.59 ns 11.2305 - 141306 B Add_One 100000 EventCoordinatorConcurrentDictionary ? 67.44 ns 0.302 ns 0.252 ns 67.39 ns 0.0107 - 136 B Add_N 100000 EventCoordinatorMutex 10 4,724.82 ns 75.103 ns 80.359 ns 4,705.35 ns 0.2899 - 3643 B Add_N 100000 EventCoordinatorMutex 100 18,839.55 ns 581.042 ns 1,713.215 ns 18,361.50 ns 1.1597 - 14548 B Add_N 100000 EventCoordinatorMutex 1000 39,517.13 ns 788.836 ns 1,422.433 ns 39,263.24 ns 8.7280 0.1221 108882 B Add_One 100000 EventCoordinatorMutex ? 27.40 ns 0.061 ns 0.051 ns 27.41 ns 0.0083 - 104 B Add_N 100000 EventCoordinatorReaderWriterLock 10 6,893.68 ns 239.329 ns 698.135 ns 6,715.11 ns 0.3357 - 4272 B Add_N 100000 EventCoordinatorReaderWriterLock 100 22,724.46 ns 647.550 ns 1,909.316 ns 22,648.35 ns 1.4343 - 18126 B Add_N 100000 EventCoordinatorReaderWriterLock 1000 NA NA NA NA NA NA NA Add_One 100000 EventCoordinatorReaderWriterLock ? 59.38 ns 0.193 ns 0.171 ns 59.38 ns 0.0107 - 136 B <p>Benchmarks with issues:   Benchmarks.Add_N: DefaultJob [InitialSize=100, Implementation=EventCoordinatorReaderWriterLock, amount=100]   Benchmarks.Add_N: DefaultJob [InitialSize=100, Implementation=EventCoordinatorReaderWriterLock, amount=1000]   Benchmarks.Add_N: DefaultJob [InitialSize=10000, Implementation=EventCoordinatorReaderWriterLock, amount=100]   Benchmarks.Add_N: DefaultJob [InitialSize=10000, Implementation=EventCoordinatorReaderWriterLock, amount=1000]   Benchmarks.Add_N: DefaultJob [InitialSize=100000, Implementation=EventCoordinatorReaderWriterLock, amount=1000]</p>"},{"location":"introduction/deployment.html","title":"Deployment","text":"<p>The deployment process of the whole stack depends on multiple individual pieces of deployment process. You will rarely (if ever) have to perform all of these at once. Furthermore there is automation that will cover a lot of these things for you. To exaplin the whole process we have to distinguish between:</p> <ol> <li>Deploying infrastructure to cloud provider</li> <li>Deploying backend api to the chosen infrastructure</li> <li>Deploying the frontend app to google store</li> </ol>"},{"location":"introduction/deployment.html#deploying-infrastructure-to-cloud-provider","title":"Deploying infrastructure to cloud provider","text":"<p>At the moment we have chosen to use AWS as our cloud provider and our pulumi config is implemented to support that. To go in a little bit more detail we have chosen the AWS Fargate where we host our containers. The complete architecture can be seen here.</p> <p>After making changes to the <code>infra/backend</code> project, be it the configuration changes in <code>Pulumi(.&lt;env&gt;).yaml</code> or adding and removing resources in <code>Program.cs</code> you should follow the same approach we have for all other changes:</p> <ol> <li>Make a separate branch</li> <li>Make a commit and push the branch</li> <li>Wait for the checks to pass</li> <li>Ask for a review and merge the code</li> </ol> Automation does help! <p>One of the checks that are run on the pull request is checking the preview of changes that will be made to the infrastructure. If you made any changes to the infra and if they pass you will get a report by github bot that should look something like this:</p> <p></p> <p>This will help the team and the reviewers (and you) to quickly inspect how the changes look like and if the pull request does what it is supposed to. Even if we try and push all our efforts in making the whole process bug proof they will occur but all these steps are bettering the Sotex-box ecosystem!</p> <p>Once the code gets merged to the main branch your changes will be successfully pushed to <code>staging</code> environment.</p> <p>TODO</p> <p>Explain the process for deploying on <code>production</code>. There should be automation handling this</p>"},{"location":"introduction/deployment.html#deploying-backend-api-to-the-chosen-infrastructure","title":"Deploying backend api to the chosen infrastructure","text":"<p>TODO</p> <p>Explain the process of deploying the backend api to cloud. There should be automation handling this</p>"},{"location":"introduction/deployment.html#deploying-the-frontend-app-to-google-store","title":"Deploying the frontend app to google store","text":"<p>TODO</p> <p>Explain the process of deploying the frontend app to google store in high level. There should be automation handling this</p>"},{"location":"introduction/development-process.html","title":"Development process","text":"<p>TODO</p> <p>Explain how can one go from development to different environments</p>"},{"location":"introduction/hardware-design.html","title":"Hardware design","text":"<p>TODO</p> <p>Explain the hardware design choices</p>"},{"location":"introduction/software-arch.html","title":"Software architecture","text":"<p>TODO</p> <p>Explain software architecture choices</p>"},{"location":"introduction/system-overview.html","title":"System overview","text":"<p>TODO</p> <p>Explain the whole system overview in higher level</p>"},{"location":"load-tests/load-test-results-connections.html","title":"Device connections","text":"<p>This test is meant to simulate the real world worst case scenario for our backend. The backend is fine-tuned for 1000 devices connected to SSE endpoint. In this test we are running 1200 connections to the endpoint for 1 hour. The goal of the test was to see if we would drop any packets for continous 1 hour and if the application would be able to handle 120% load of the fine-tune desired load.</p>"},{"location":"load-tests/load-test-results-connections.html#setup-hardware","title":"Setup - Hardware","text":"<p>The backend, alongisde all other production services like <code>nginx</code>, <code>prometheus</code> and <code>grafana</code> were deployed to AWS EC2 instance. For this test we chose the following configuration:</p> Model vCPU Memory (GiB) Instance Storage (GB) Network Bandwidth (Gbps) EBS Bandwidth (Mbps) c5.large 4 8 EBS-Only Up to 10 Up to 4750 <p>To see and compare all the choices there are on AWS consolidate this link.</p>"},{"location":"load-tests/load-test-results-connections.html#setup-software","title":"Setup - Software","text":"<p>The code for all the tools we run in production was checked in the repository and pulled on ec2 instance using <code>git</code>. Apart from that we've used <code>docker</code> and its <code>docker compose</code> to run the stack. The service that was open to the internet was <code>nginx</code> which is an entrypoint to the whole system.</p>"},{"location":"load-tests/load-test-results-connections.html#test-run","title":"Test run","text":"<p>The test was run from two physical machines, both laptops of roughly the same architecture <pre><code>Ubuntu 22.04.4 LTS (Jammy Jellyfish)\n12th Gen Intel Core i7-12700H, 1 CPU, 20 logical and 14 physical cores\n</code></pre> Each laptop simulated 600 connections to the backend for 1 continous hour using <code>k6</code>. The command that was used to run the test from one laptop is: <pre><code>k6 run load-tests/sse/connectionTest.js --vus 600 --iterations 600 --duration 60m --env BACKEND_URL=http://ec2-18-156-71-117.eu-central-1.compute.amazonaws.com  --env NOOP_INTERVAL=15 --env SECONDS=3600 --env PREFIX=&lt;laptop-prefix&gt;\n</code></pre></p>"},{"location":"load-tests/load-test-results-connections.html#math","title":"Math","text":"<p>Since we are limited by the lack of support for propper SSE handler in <code>k6</code> we had to take into account the amount of load this will take on the laptop itself so it doesn't get killed by Linux OOM Killer. <pre><code>noop = \"data: 0\\n\\n\"                                      # Data that comes with each noop\nnoopSize = noop.length * 1B = 9B                          # Size of 1 noop in Bytes\ntotalSeconds = 3600                                       # Desired seconds for a test to run\ntestShutdown = totalSeconds - 5 = 3595                    # Shutdown signal for one test\nnoopInterval = 15                                         # 1 noop per 15 seconds\nvus = 600                                                 # Total VUs per laptop\nexpectedNoopsPerVu = testShutdown / noopInterval = 239    # Total expected noops per test\ntotalNoops = vus * expectedNoopsPerVu = 143400            # Total noops per laptop\ntotalMemory = totalNoops * noopSize = 1290600B ~ 1.23MB   # Total memory consumption per laptop\n</code></pre></p>"},{"location":"load-tests/load-test-results-connections.html#results","title":"Results","text":"<p>After 1 hour of testing the results were as follows: <pre><code>     \u2713 Delete status was 200\n     \u2713 Get status was 200\n     \u2713 Received all messages\n\n     checks.........................: 100.00% \u2713 1800     \u2717 0  \n     data_received..................: 2.9 MB  817 B/s\n     data_sent......................: 207 kB  58 B/s\n     http_req_blocked...............: avg=797.26ms min=26.53ms med=1.04s   max=3.13s  p(90)=2.07s    p(95)=2.08s  \n     http_req_connecting............: avg=796.1ms  min=26.41ms med=1.04s   max=3.13s  p(90)=2.07s    p(95)=2.08s  \n     http_req_duration..............: avg=29m57s   min=27.01ms med=29m57s  max=59m58s p(90)=59m56s   p(95)=59m56s  \n       { expected_response:true }...: avg=29m57s   min=27.01ms med=29m57s  max=59m58s p(90)=59m56s   p(95)=59m56s  \n     http_req_failed................: 0.00%   \u2713 0        \u2717 1200\n     http_req_receiving.............: avg=29m51s   min=20.04\u00b5s med=29m50s  max=59m44s p(90)=59m43s   p(95)=59m43s  \n     http_req_sending...............: avg=155.16\u00b5s min=15.37\u00b5s med=92.26\u00b5s max=1.41ms p(90)=310.81\u00b5s p(95)=508.01\u00b5s\n     http_req_tls_handshaking.......: avg=0s       min=0s      med=0s      max=0s     p(90)=0s       p(95)=0s  \n     http_req_waiting...............: avg=6.67s    min=26.8ms  med=7.09s   max=14.5s  p(90)=13.75s   p(95)=13.93s  \n     http_reqs......................: 1200    0.333439/s\n     iteration_duration.............: avg=59m56s   min=59m55s  med=59m56s  max=59m58s p(90)=59m57s   p(95)=59m58s  \n     iterations.....................: 600     0.166719/s\n     vus............................: 31      min=31     max=600\n     vus_max........................: 600     min=600    max=600\n\n\nrunning (0h59m58.9s), 000/600 VUs, 600 complete and 0 interrupted iterations\ndefault \u2713 [======================================] 600 VUs  0h59m58.9s/1h0m0s  600/600 shared iters\n</code></pre> Result interpretation:</p> <ul> <li><code>data_received</code>: In the math above we calculated roughly 2 MB of data expected, but that is only the size of the bodies received. We couldn't take into account all the headers that will arive since AWS places custom headers for each request and that accounts for the remaining 0.9 MB of data</li> <li><code>http_req_duration</code>: This was the goal of the test, we see that for <code>p(95)</code> (the 95th procentile, or in other words for 95% of requests) the request lasted the whole 59m56s which aligns with the test <code>testShutdown</code> of 3595 seconds, 1 second miss is due to all tests requesting their own shutdown in parallel and backend has the dictionary behind a <code>Mutex</code> which causes the delay.</li> <li><code>iteration_duration</code>: This confirms the <code>http_req_duration</code> and ensures that the test doesn't do anything apart from preparation, test and assertion, where preparation and assertion for <code>p(95)</code> lasted for roughly 2 seconds.</li> </ul> <p>The whole test metrics for 1 laptop can be found in <code>test-output.json</code></p> CPU utilization for the test duration <p>For the whole test duration we can see that there were 2 spikes in the backend CPU utilization. The average utilization was around <code>14.3%</code>. Explaination of the spikes:</p> <ul> <li>spike at <code>20:52</code>: related to gen 1 garbage collection</li> <li>spike at <code>21:17</code>: roughly the time where all the tests ended which created 1200 http delete requests to the backend</li> </ul> Memory consumption <p>During the whole test memory consumed <code>500MB</code> of memory maximum for all connected devices. If you take a look at the hardware spec this means that there is plenty of resources left for <code>nginx</code>, <code>prometheus</code> and <code>grafana</code> to work alongside future tuning of database and serving other resources from the backend.</p>"},{"location":"observability/index.html","title":"Overview","text":"<p>This document provides an overview of how our application leverages observability techniques to provide insight into its health and performance. We utilize both logging and metrics to offer comprehensive visibility into its operation.</p>"},{"location":"observability/index.html#logging","title":"Logging","text":"<p>Our application writes structured logs to standard error (stderr) in JSON format. This format facilitates parsing and analysis by downstream systems. We plan to employ rolling indices in Elasticsearch to ensure efficient storage and retrieval of these logs.</p> <p>Key Features:</p> <ul> <li>Structured JSON Format: Enables efficient parsing and querying and also provides flexibility to easily consume logs with tools such as jq</li> <li>Rolling Indices in Elasticsearch: Will be added to ensure logs gathering</li> </ul>"},{"location":"observability/index.html#metrics","title":"Metrics","text":"<p>Our application exposes operational metrics at the <code>/metrics</code> endpoint, following the Prometheus format. This allows scraping and visualization of key performance indicators within Prometheus and other compatible tools.</p> <p>Key Features:</p> <ul> <li>Prometheus Format: Widely adopted and supported by various monitoring tools.</li> <li>/metrics Endpoint: Provides easy access to essential performance indicators.</li> <li>Visualization &amp; Analysis: Enables real-time and historical performance observability and monitoring.</li> <li>Alerting: Enables real-time alerting based on occured problems.</li> </ul>"},{"location":"observability/index.html#benefits","title":"Benefits","text":"<p>By embracing observability practices, we achieve the following benefits:</p> <ul> <li>Enhanced Troubleshooting: Logs and metrics facilitate pinpointing issues and understanding application behavior.</li> <li>Performance Monitoring: Gain insights into resource utilization and identify potential bottlenecks.</li> <li>Proactive Maintenance: Identify and address potential problems before they impact users.</li> <li>Improved User Experience: Real-time feedback allows for quick response to user-facing issues.</li> </ul> <p>This overview provides a high-level introduction to our observability strategy.</p>"},{"location":"observability/custom-metrics.html","title":"Custom metrics","text":"<p>We serve various metrics that are used to monitor the stack and to evaluate whether the application meets its required performance requirements. We have included a lot of metrics related to the overall application ecosystem. Backend servers metrics related to: * ASP.NET Core instruments * .NET Runtime instruments * Process instruments * Custom metrics</p> <p>Most of the metrics explaination can be found on this blog post.</p>"},{"location":"observability/custom-metrics.html#other-metrics","title":"Other metrics","text":"<p>Alongisde those we have kept tract of some things on our own and those are:</p> Metric name Unit Tags Description <code>sotex_web_devices_num_total</code> total <ul><li><code>id</code>: device id</li><ul> Represents the number of connected devices to backend. It is also used to see if  the device is currently connected or not <code>sotex_web_sent_bytes_total</code> byte <ul><li><code>id</code>: device id</li><ul> Represents the total amout of bytes sent from backend to frontend via SSE during one connection"},{"location":"repository/index.html","title":"Overview","text":"<p>This overview page serves as your guide to navigating the exciting world of Sotex-box's codebase. Whether you're a seasoned developer joining the team or a curious explorer, we aim to provide a clear understanding of our project's structure and its inner workings.</p>"},{"location":"repository/index.html#repository-layout","title":"Repository layout","text":"<p>The Sotex-box repository is organized into well-defined directories, each with a specific purpose:</p> <pre><code>.\n\u251c\u2500\u2500 .config # Folder that contains config files, e.g. config for dotnet tools\n\u251c\u2500\u2500 .dockerignore # File that contains all the things that docker should include in the build process\n\u251c\u2500\u2500 .editorconfig # File that specifies how should files be formated\n\u251c\u2500\u2500 .env.template # File that specifies all environment variables for docker\n\u251c\u2500\u2500 .github # Folder with workflows\n\u251c\u2500\u2500 .pre-commit-config.yaml # Precommit configuration\n\u251c\u2500\u2500 CODEOWNERS # File containing the codeowners of certain parts of the repository\n\u251c\u2500\u2500 Makefile # Make file that contains some quick-access shortcuts maintained by the team\n\u251c\u2500\u2500 README.md # Readme that is rendered for github repository\n\u251c\u2500\u2500 android # Folder that contains android code that we use\n\u251c\u2500\u2500 distribution # Folder containing images for our stack\n\u251c\u2500\u2500 docker-compose.yaml # Development stack\n\u251c\u2500\u2500 docs # Folder containing all the documentation that is used to power this site\n\u251c\u2500\u2500 dotnet # Folder containing all the dotnet code we use\n\u251c\u2500\u2500 infra # Folder containing all IAAC code we use\n\u251c\u2500\u2500 load-tests # Folder containing all load tests\n\u251c\u2500\u2500 mkdocs.yaml # Configuration file for this site\n\u251c\u2500\u2500 pixi.lock # Lock of dependencies for pixi project\n\u251c\u2500\u2500 pixi.toml # Pixi project specification\n\u251c\u2500\u2500 requirements.txt # Autoexported file for python used for docker images based of scripts\n\u2514\u2500\u2500 sotex-box.sln # Solution file that manages dotnet code\n</code></pre>"},{"location":"repository/index.html#code-related-information","title":"Code-Related information","text":"<p>Some random facts about the code:</p> <ul> <li>Each change should be carried out by a PR into a <code>main</code> branch.</li> <li>For each PR write a meaningful description. You will be provided with a pull request template which you should extend.</li> <li>Take your time with the PR. Don't skip and don't rush. We emphasize on quality rather than quantity.</li> <li>Add tests. From unit and integration, to end-to-end and smoke tests.</li> <li>Use precommit hooks. They will help you detect problems with your code even before you push the code.</li> <li>There is CI pipelines setup which ensure code quality and automate deployments. Their passing is a must for any PR to be accepted.</li> </ul>"},{"location":"repository/index.html#getting-started","title":"Getting started","text":"<p>To kickstart your journey with Sotex-box development:</p> <ol> <li>Clone the repository</li> <li>Setup your environment: Follow instructions from Working with repository guide</li> <li>Run the tests: Verify the project's functionallity by running the tests <pre><code>make dotnet-tests\n</code></pre></li> <li>Explore the code: Dive into the source code and find out more about parts that are of your interest</li> <li>Contribute: Make a branch, follow code-related and merge your changes</li> </ol> <p>We hope this overview serves as a valuable starting point for your exploration of the Sotex-box codebase. We  encourage you to actively contribute and help us push the boundaries of this exciting project!</p>"},{"location":"repository/working-with-repository.html","title":"Working with repository","text":""},{"location":"repository/working-with-repository.html#local-tools","title":"Local tools","text":"<p>You need a couple of tools to be able to fully work with this repository:</p> <p>Tip for windows users</p> <p>Even if you are using windows to get the best development experience its advised to use WSL!</p> # Name Version Windows Linux Mac 1 python ^3.9 \ud83d\udd17 \ud83d\udd17 \ud83d\udd17 2 pixi ^0.22.0 \ud83d\udd17 \ud83d\udd17 \ud83d\udd17 3 precommit ^3.6.0 \ud83d\udd17 \ud83d\udd17 \ud83d\udd17 4 dotnet ^8.0.1 \ud83d\udd17 \ud83d\udd17 \ud83d\udd17 5 pulumi ^3.107.0 \ud83d\udd17 \ud83d\udd17 \ud83d\udd17 6a docker (suggested) ^23.0.5 \ud83d\udd17 \ud83d\udd17 \ud83d\udd17 6b podman ^4.8.3 \ud83d\udd17 \ud83d\udd17 \ud83d\udd17 7 java ^21.0.2 \ud83d\udd17 \ud83d\udd17 \ud83d\udd17 8 sdkmanager ^12.0 \ud83d\udd17 \ud83d\udd17 \ud83d\udd17 9 gradle ^4.4.1 \ud83d\udd17 \ud83d\udd17 \ud83d\udd17 10 kotlin ^1.9.21 \ud83d\udd17 \ud83d\udd17 \ud83d\udd17 11 flutter ^3.19.1 \ud83d\udd17 \ud83d\udd17 \ud83d\udd17 12 k6 ^0.49.0 \ud83d\udd17 \ud83d\udd17 \ud83d\udd17 13 android platform tools latest \ud83d\udd17 \ud83d\udd17 \ud83d\udd17 <p>For more details about flutter installation, visit the android part of the docs.</p> <p>We will maintain a <code>Makefile</code> where we will try to link as much actions as possible. Although some cases may be left uncovered.</p> <p>Once you install the tools you should setup precommit hooks which help us maintain the code quality and runs tests on commit. Run the bellow command to do so. <pre><code>pre-commit install\n</code></pre></p> <p>After that you can use <code>make</code> to run things from the repository. Some things that are not implemented require specific commands but those are a niece topic still.</p>"},{"location":"repository/working-with-repository.html#running-the-fullstack-locally","title":"Running the fullstack locally","text":"<p>In order to support having 0 setup locally we've setup <code>docker-compose.yaml</code> that serves as a quick run and test tool for the stack. Most of the time it is expected to use this in order to work with the solution as a whole. Some of the tests require you to run the whole stack so you can see how the new feature you are implementing plays against the load. Some things cannot be reproduced locally (for example networking issues) but still you can see have a general idea of whether your improvement solves a problem or not.</p> <p>Environment variables</p> <p>In order to tie everything together we are using environment variables. There is a <code>.env.template</code> which specifies all environment variables that we set. In order to start the stack you should first edit variables to suit your own. Start with <pre><code>cp .env.template .env\n</code></pre></p> <p>To start the stack simply run: <pre><code>make compose-up\n</code></pre> After you are done you can remove the stack by running <pre><code>make compose-down\n</code></pre></p>"},{"location":"repository/CI/index.html","title":"Overview","text":"<p>From the day one we've invested time into setting up continous integration and continous deployment strategies to ensure a smooth sailing process from development to staging to production. There are multiple things that happen if you change code in some way and these changes aim to help you automate the transition from one environment to another ensuring greater success rate and making those processes as transparent as possible.</p>"},{"location":"repository/CI/index.html#dotnet-checks","title":"Dotnet checks","text":"<p>Defined in <code>.github/workflows/dotnet.yaml</code> we check multiple things related to code quality, unit testing, integration testing and getting notified of anything failing as soon as possible.</p> <p>Code quality tips &amp; tricks</p> <p>If you want to ensure that your code passes code quality requirements be sure to enable <code>pre-commit</code> by installing it! To view the full process on how to setup your repository to ensure success follow the \"Working with repository\" guide! Some things that can fail on CI are already automatically fixable with these.</p> <p>Apart from assuring code quality and test passage, this pipeline ensures that we can build a docker image from the changes you've introduced. Be aware that if the merge request doesn't involve any changes to the stack related to the docker images it won't trigger the build.</p> <p>Right now we are using alipne linux as operating system to base our images off of which results in images being really small.</p> <p>On top of that, when your code gets merged into <code>main</code> branch we will automatically push the image to our repositories packages. You can find all the exporter packages on ghcr. After having an image tagged and built we will go ahead and update staging with the new version we built in ci.</p> <p>TODO</p> <p>Extend this once we have production setup and have a process in place.</p>"},{"location":"repository/android/index.html","title":"Overview","text":"<p>We use Dart and Flutter for the device frontend.</p>"},{"location":"repository/android/index.html#high-level-frontend-overview","title":"High level frontend overview","text":"<p>Two Flutter applications support this frontend. Launcher is going to be our environment UI which will take care of the things like splashscreen, device setup, device settings, and opening the Sotex Box app. Sotex Box is an actual app which will provide the targeted media services.</p>"},{"location":"repository/android/index.html#emulation","title":"Emulation","text":"<p>You are free to use either software emulation or android supported hardware. To setup the local software emulator you can use make: <pre><code>make setup-emulator\n</code></pre> You only have to do this once, but for the sake of simplicity and possibility of platform version changes it is added as a command.</p> <p>Before running any other part of the frontend you have to spin up the emulator. <pre><code>make run-emu\n</code></pre></p> <p>Emulator might not be in the optimal orientation, so you can rotate it using: <pre><code>make rotate-emu\n</code></pre></p> <p>Emulator runs in 1080p resolution however to have it in full-screen you can extend the window by dragging out the upper left corner.</p> <p>Emulator might not support back and home buttons. To remedy these go to: <code>~/.android/avd/android_tv.avd/config.ini</code>.</p> <p>Set <code>hw.dpad = yes</code> Set <code>hw.keyboard = yes</code></p> <p>In order to run a particular part of the frontend you can use make:</p> <pre><code>make run-launcher\n</code></pre>"},{"location":"repository/android/index.html#testing","title":"Testing","text":"<p>There is a heavy emphasis on both unit and integration testing. This doesn't mean we follow TDD but we just want to ensure the code going to an environment does what it is supposed to. On top of that each bug or incident we receive we will create a test for it making sure that the same bug doesn't repeat itself.</p> <p>Sometimes when writing tests developers tend to adhere to DRY principles too much. Try to aim for maintainable and readable tests rather than adhere to a principle.</p> <p>To run all the tests you can use <code>make</code>: <pre><code>make flutter-test\n</code></pre></p> <p>To run a particular set of tests you can use <code>make</code>: <pre><code>make flutter-test-launcher\n</code></pre></p> <p>If you have problems setting it up, look at this troubleshooting guide.</p>"},{"location":"repository/android/troubleshoot.html","title":"CMDLINE-TOOLS","text":"<p>When setting up the cmdline-tools, the directory containing it has to have a specific subpath.  Please look at this stack overflow answer.</p>"},{"location":"repository/dotnet/index.html","title":"Overview","text":"<p>We use .NET 8 and that is the technology we use mainly for the backend side.</p>"},{"location":"repository/dotnet/index.html#directory-layout","title":"Directory layout","text":"<pre><code>dotnet\n\u251c\u2500\u2500 backend # Main api code\n\u251c\u2500\u2500 benchmarks # Benchmarks used for various performance testing\n\u251c\u2500\u2500 e2e-tester # End to end tester related to backend stack\n\u251c\u2500\u2500 integration-tests # Integration tests related to backend\n\u251c\u2500\u2500 model # Place where we keep core models and contracts used to transmit data\n\u251c\u2500\u2500 persistence # Place where we keep database specific code\n\u251c\u2500\u2500 sse-handler # Library for handling server sent events\n\u2514\u2500\u2500 unit-tests # Unit tests for libraries\n</code></pre>"},{"location":"repository/dotnet/index.html#high-level-backend-overview","title":"High level backend overview","text":"<p>The backend is implemented in ASP.NET Core framework and is built as an API server that is used for serving information to both real users and hardware devices. It is a glue that ties together information and orchestrates events. Here you will find the controllers that receive requests for managing resources that the server maintains.</p>"},{"location":"repository/dotnet/index.html#benchmarks","title":"Benchmarks","text":"<p>Benchmarks are a place where we measure how different implementations compare to one another giving us a way to easily devise which implementation should make it to prod. Bare in mind that we cannot measure every singe bit of the code, but we try to keep a high threashold for performance and having these metrics are of great importance. Not everything can be measured with benchmarks and for some metrics we need a live environment so there is a plan to add benchmarks in form of tests that will measure how well the server performs during heavy load.</p>"},{"location":"repository/dotnet/index.html#testing","title":"Testing","text":"<p>There is a heavy emphasis on both unit and integration testing. This doesn't mean we follow TDD but we just want to ensure the code going to an environment does what it is supposed to. On top of that each bug or incident we receive we will create a test for it making sure that the same bug doesn't repeat itself.</p> <p>Sometimes when writing tests developers tend to adhere to DRY principles too much. Try to aim for maintainable and readable tests rather than adhere to a principle.</p> <p>To run all the tests you can use <code>make</code>: <pre><code>make dotnet-tests\n</code></pre></p>"},{"location":"repository/dotnet/index.html#unit-testing","title":"Unit testing","text":"<p>For unit testing, we use MSTest. All the tests are in <code>unit-tests</code> project and can be run with <code>make</code>: <pre><code>make dotnet-unit-tests\n</code></pre></p>"},{"location":"repository/dotnet/index.html#integration-testing","title":"Integration testing","text":"<p>For integration testing, we use XUnit. Apart from that we use Testcontainers to mock the dependencies of our system. All the tests are in <code>integration-tests</code> and can be run with <code>make</code>: <pre><code>make dotnet-integration-tests\n</code></pre></p> <p>Keep in mind that we cannot cover all the possible scenarious with integration tests that can occur in real life scenarious. If we added authentication and authorization to all controllers we don't need to have a test for EVERY controller saying something in the lines of pseudo code: <pre><code>[Fact]\npublic async Task Should_NotDoStuff_AuthError() {\n    // Arange\n    var client = _factory.CreateClient();\n\n    // Act\n    var response = await client.GetAsync(\"protected/resource\");\n\n    // Assert\n    response.StatusCode.ShouldBeIn(new [] { HttpStatusCode.FORBIDDEN, HttpStatusCode.UNAUTHORIZED })\n}\n</code></pre></p> <p>Try to write tests that cover the business logic and add value to the codebase ensuring that future changes to some code don't accidentally break the logic. Writing meaningless tests results in denied PR's.</p>"},{"location":"repository/dotnet/index.html#load-testing","title":"Load testing","text":"<p>For load testing, we use k6. Since we know the goal we want to reach we need these kinds of tests to ensure that our implementation can work in desired circumstances. Keep in mind that our solution doesn't have to, and probably won't work for parameters greater than defined by these tests. If backend can work under load specified by these tests it is expected that it will work in production as well.</p> <p>For this tool there is no <code>make</code> action. One needs to run it on their own with their own desired parameters. To run the tests you should:</p> <ol> <li>The stack needs to run: <code>make compose-up</code></li> <li>Run the <code>k6</code> tool: <pre><code>k6 run load-tests/sse/connectionTest.js --vus 10 --iterations 10 --env BACKEND_URL=&lt;backend-url&gt; --env NOOP_INTERVAL=&lt;noop-interval&gt; --env SECONDS=&lt;seconds&gt;\n</code></pre></li> </ol> <p>Parameter explaination:</p> <ol> <li><code>--vus</code>: number of virtual users to run. Each virtual user will be a separate goroutine and will create a connection to the backend.</li> <li><code>--iterations</code>: number of iterations to run. It should be atleast equal to <code>--vus</code>. If <code>iterations</code> is larger than <code>vus</code> then some users will run the test logic twice.</li> <li><code>--env</code>: represent environment variables for the test</li> </ol> <p>Environment variable explaination:</p> <ol> <li><code>BACKEND_URL</code>: depending on how you run backend this option can and will vary. If you run <code>make run-backend</code> this should have value of <code>http://localhost:5029</code>. If you run with <code>make compose-up</code> this should have value of <code>http://localhost:8080</code>. If you want to run tests against staging you should provide the dns name of staging and so on.</li> <li><code>NOOP_INTERVAL</code>: specified noop interval in seconds. This number represents on how many seconds does the server send a <code>noop</code> signal. By default the server sends a <code>noop</code> each 15s so if you haven't provided an override then this parameter should have the value of <code>15</code>.</li> <li><code>SECONDS</code>: the duration of the test in seconds. If you want to run a test for 10 minutes you should provide value of <code>600</code> here.</li> </ol> <p>Example run for stack run with <code>make compose-up</code> for <code>30</code> users for <code>10</code> minutes: <pre><code>k6 run load-tests/sse/connectionTest.js --vus 30 --iterations 30 --env BACKEND_URL=http://localhost:8080 --env NOOP_INTERVAL=15 --env SECONDS=600\n</code></pre></p> <p>Important notes:</p> <p>Keep in mind that this test can be quite heavy. The test runs the threads in parallel and keeps them open for <code>SECONDS</code> amount of time. Since the server sends the <code>noop</code> signal at fixed rates you can easily calculate how much data will the test create. The noop signal looks like <code>data: 0\\n\\n</code> and by default the server sends 1 noop each 15 seconds. For a detailed math that one should keep in math visit this page.</p> <p>What this means is that one should be careful on how many users he spins and how long does the test last since it can be heavy on the machine running the test itself.</p>"},{"location":"repository/dotnet/index.html#e2e-testing","title":"E2E testing","text":"<p>For e2e testing we use a custom written framework that is tailored for our stack. It is making use of dind. Since e2e tests cover the broader use cases of a certain software feature we need to treat them separately. The goal is to test complete use-cases in a dedicated environment. The tester has the ability to spin up multiple instances that will be used for running tests against. Usually e2e tests are expensive and could take some time but their mission is to catch production level bugs so they don't have to be run as often.</p> <p>To run e2e tests you simply run: <pre><code>make dotnet-e2e-tests\n</code></pre> Since you will be running a couple of environment instances locally it is expected to see a higher cpu usage like the following: <pre><code>docker stats --no-stream\nCONTAINER ID   NAME                                                       CPU %     MEM USAGE / LIMIT     MEM %     NET I/O           BLOCK I/O       PIDS\n437f4a57c351   e2e-tester-2                                               2.13%     749.1MiB / 31.06GiB   2.35%     75.6kB / 41kB     108MB / 182MB   450\nfe8eccc6fb95   e2e-tester-1                                               20.01%    964.1MiB / 31.06GiB   3.03%     151kB / 43kB      331MB / 231MB   452\n8a68ef0e37ed   e2e-tester-0                                               1.91%     972.7MiB / 31.06GiB   3.06%     80.2kB / 51kB     341MB / 230MB   452\n31dce92219b1   testcontainers-ryuk-2cc9deca-ab4e-46a3-a8eb-2792f0b0cb73   0.00%     9.723MiB / 31.06GiB   0.03%     10.6kB / 4.62kB   6.51MB / 0B     9\n</code></pre> At the end of the run there will be an output that present the overview of the run:</p> E2E test output in console <p>Some of the tests are allowed to fail, here there is a grafana ping that is failing due to misconfiguration for development testing purposes which is not needed for this environment. Tests marked as allowed to fail won't result in overall outcome of the test run. Usually these will be flaky tests.</p> <p>There isn't a general rule on when to write an e2e test and when not to. There are some things that are mission critical and should be covered with an e2e test but it really depends what is being tested. If the logic is being tested usually it is okay to write just a unit test. If there is a lot of services talking it is better to have an integration test. If there is a business goal that needs to be fulfilled and there is are timings that need to be tested (for e.g. cron firing) there is a need for an e2e test.</p> <p>A good example of an e2e test is a <code>Create ad</code> test that has to test what happens when the user creates an ad, receives a link from backend to which to upload his ad to and then wait for the backend to store additional info that comes from the blob storage via a queue.</p>"},{"location":"repository/infra/index.html","title":"Overview","text":"<p>This document provides a high-level overview of our infrastructure and IaC practices. It will be continously updated as our infrastructure evolves.</p> <p>To dive deeper in concrete architecture that is deployed you should go and check out our software architecture document.</p>"},{"location":"repository/infra/index.html#pulumi-tool","title":"Pulumi tool","text":"<p>In order to help automate creation and deployment of environments we take advantage of pulumi. The biggest benefit that we see is the ability to use any programming language and easily manage cloud resources. Since most of our backend stack is dotnet related we've gone and implemented it using c#. All the code that we use to deploy our resources is in <code>infra/backend</code> project.</p> <p>In <code>Pulumi.yaml</code> we have defined all the configuration values that our stack uses and we've gone and provided default values that we use for <code>staging</code> environment there. All of the values can be overriden in <code>Pulumi.&lt;env&gt;.yaml</code> file if necessary.</p>"},{"location":"repository/infra/index.html#how-we-map-configuration","title":"How we map configuration","text":"<p>Pulumi provides us with a mechanism for fetching configuration from its <code>Pulumi(.&lt;env&gt;).yaml</code> files. However its not strictly typed. You are able to make mistakes easily and you will notice that only once you deploy a stack. On top of that if you write something like <pre><code>var value = config.RequireString(\"networking-vpc-cidrBlock\");\n</code></pre> you will end up with a lot of constant strings that you have to manage as well. For now our project for IaC only contains one file with resources, named <code>Program.cs</code> but in the future if we want to support deploying to multiple clouds and reuse some parts of configuration it will be unmanagable. One more issue is that when typing out the name of the config value you want to get you can write anything and there is no lsp support.</p> <p>To tackle observed problems we've implemented a <code>config-translator</code> which is a library that can map pulumi config into classes and after that you can use them with ease.</p>"},{"location":"repository/infra/index.html#creating-your-configuration","title":"Creating your configuration","text":"<p>If you want to add a custom value you should go to <code>Pulumi.yaml</code> and add the definition for your value: <pre><code>...\n    sotex-box:my-custom-setting:\n        description: A custom setting added for demo purposes\n        default: \"custom value\"\n        type: string\n...\n</code></pre> after that you can register it in <code>MappedConfig.cs</code> with adding a two lines in the class: <pre><code>public class MappedConfig {\n    ...\n    [String(\"my-custom-setting\")]\n    public int MyCustomValue { get; set; }\n}\n</code></pre> Having done all this you will be able to use your value within the infrastructure deployment development, testing and deployment process.</p>"}]}